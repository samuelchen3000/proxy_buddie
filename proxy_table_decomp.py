# -*- coding: utf-8 -*-
"""proxy_table_decomp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vQLwUW8Kounfn9LEMIREOc4TcuA500MO
"""

# If needed:
# !pip install pandas pyarrow

import os
import re
import unicodedata
import pandas as pd

# ---- Config ----
INPUT_CSV = "deduced_goog_proxy_data.csv"  # after uploading
OUT_DIR = "extracted_tables_2"
INDIV_DIR = os.path.join(OUT_DIR, "individual_tables")
os.makedirs(INDIV_DIR, exist_ok=True)

# ---- Helpers ----
def normalize_header_name(s: str) -> str:
    """Trim, collapse spaces, strip zero-width / control chars, and normalize unicode."""
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", str(s))
    # remove zero-width characters and control chars
    s = "".join(ch for ch in s if not unicodedata.category(ch).startswith("C"))
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    return s

def make_unique_columns(cols):
    """Ensure unique column labels by appending .1, .2, ... as needed after normalization."""
    normed = [normalize_header_name(c) for c in cols]
    seen = {}
    out = []
    for c in normed:
        if c in seen:
            seen[c] += 1
            out.append(f"{c}.{seen[c]}")
        else:
            seen[c] = 0
            out.append(c)
    return out

def is_separator_line(row: str) -> bool:
    test = row.replace(" ", "")
    # common markdown separator: |---|:---:|---:|
    return bool(re.fullmatch(r"\|?:?-{3,}:?(\|:?-{3,}:?)+\|?", test))

def split_pipe_row(row: str):
    row = row.strip()
    if row.startswith("|"): row = row[1:]
    if row.endswith("|"): row = row[:-1]
    return [cell.strip() for cell in row.split("|")]

def parse_pipe_table(raw: str) -> pd.DataFrame:
    """
    Parse a markdown-style table with '|' column separators.
    Handles an optional dashed header-separator line.
    """
    if not isinstance(raw, str):
        raise ValueError("Table content is not string")
    lines = [ln for ln in (raw or "").splitlines() if ln.strip()]
    rows = [ln for ln in lines if "|" in ln]
    if len(rows) < 1:
        raise ValueError("No pipe rows")

    header = split_pipe_row(rows[0])
    data_start = 1
    if len(rows) >= 2 and is_separator_line(rows[1]):
        data_start = 2

    data = [split_pipe_row(r) for r in rows[data_start:]]
    # normalize row lengths to header length
    fixed = []
    for r in data:
        if len(r) < len(header):
            r = r + [None] * (len(header) - len(r))
        elif len(r) > len(header):
            r = r[:len(header)]
        fixed.append(r)

    df = pd.DataFrame(fixed, columns=make_unique_columns(header))
    # enforce uniqueness once more (paranoia)
    df.columns = make_unique_columns(df.columns)
    return df

# ---- Pipeline ----
df = pd.read_csv(INPUT_CSV)
assert df.shape[1] >= 2, "Need at least two columns"

table_col = df.columns[1]  # 2nd column contains the tables
meta_cols = [c for c in df.columns if c != table_col]

extracted = []
catalog = []

for idx, row in df.iterrows():
    raw = row[table_col]
    if pd.isna(raw) or not isinstance(raw, str) or "|" not in raw:
        continue
    try:
        tdf = parse_pipe_table(raw)
    except Exception:
        # skip unparseable rows
        continue

    # Add metadata (prefix to avoid colliding with table headers)
    for mc in meta_cols:
        tdf[f"meta_{mc}"] = row[mc]

    # FINAL safeguard: ensure columns unique before any concat/reindex
    tdf.columns = make_unique_columns(tdf.columns)

    base = f"table_row{idx}"
    csv_path = os.path.join(INDIV_DIR, f"{base}.csv")
    pq_path  = os.path.join(INDIV_DIR, f"{base}.parquet")

    tdf.to_csv(csv_path, index=False)
    try:
        tdf.to_parquet(pq_path, index=False)  # requires pyarrow or fastparquet
    except Exception:
        pq_path = None

    extracted.append(tdf)
    catalog.append({
        "source_row_index": int(idx),
        "n_rows": int(tdf.shape[0]),
        "n_cols_including_meta": int(tdf.shape[1]),
        "csv_path": csv_path,
        "parquet_path": pq_path
    })

# Write catalog
catalog_df = pd.DataFrame(catalog)
catalog_df.to_csv(os.path.join(OUT_DIR, "catalog.csv"), index=False)

# Build stacked file
if extracted:
    # Ensure each table has unique columns before alignment (again)
    for i in range(len(extracted)):
        extracted[i].columns = make_unique_columns(extracted[i].columns)

    # Outer-union all columns
    all_cols = sorted(set().union(*[set(t.columns) for t in extracted]))
    aligned = [t.reindex(columns=all_cols) for t in extracted]  # now safe: no dup labels on t
    long_df = pd.concat(aligned, ignore_index=True)
    long_df.to_csv(os.path.join(OUT_DIR, "all_tables_with_metadata.csv"), index=False)
    try:
        long_df.to_parquet(os.path.join(OUT_DIR, "all_tables_with_metadata.parquet"), index=False)
    except Exception:
        pass

print("✅ Done. Outputs saved under:", OUT_DIR)
print("• Individual tables:", INDIV_DIR)
print("• Catalog:", os.path.join(OUT_DIR, "catalog.csv"))
print("• Stacked CSV:", os.path.join(OUT_DIR, "all_tables_with_metadata.csv"))