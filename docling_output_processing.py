# -*- coding: utf-8 -*-
"""docling_output_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VcppRAKy8VQTf6IpJirvS8zqfn4R9nDE

#version 1
"""

# Colab-ready: extract ONLY targeted sections using HEADING_PATTERNS,
# then deduplicate by CONTENT (not by header) and export to CSV.
#
# Output CSV columns:
#   section_index, category, header_raw, header_norm, match_source, content
#
# Dedupe logic:
# - Exact duplicates removed via a normalized-content SHA-1 fingerprint.
# - Optional near-duplicate removal via TF-IDF cosine similarity (toggle below).

import re, csv, hashlib
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any, Tuple

# ---------------------------
# Config
# ---------------------------
INPUT_PATH = Path("/content/goog_proxy_1_custom_output.txt")  # <-- change to your TXT
OUT_CSV    = Path("/content/targeted_sections_dedup_by_content_goog.csv")

SEARCH_CONTENT_TOO = True         # also search patterns inside section content
PREFER_HEADER_MATCH = True        # prefer header matches when both header/content match

# Remove purely structural nav headings before classification (normalized form)
STRUCTURAL_HEADINGS = {
    "back to contents",
    "table of contents",
    "alphabet 2025 proxy statement",
}

# Near-duplicate (fuzzy) removal: requires scikit-learn
USE_FUZZY_NEAR_DUPES = False
FUZZY_SIM_THRESHOLD = 0.95  # cosine similarity threshold (0..1), higher = stricter

# ---------------------------
# Your patterns
# ---------------------------
HEADING_PATTERNS = {
  "proposals": [
    r"\bproposal\s+\d+\b",
    r"\bitem\s+\d+\b",
    r"\bmatter\s+\d+\b",
    r"\bagenda\s*item\s+\d+\b",
    r"\bmatters?\s+to\s+be\s+voted\s+on\b",
    r"\bitems?\s+of\s+business\b",
  ],
  "directors_roster_or_matrix": [
    r"\bboard\s+of\s+directors\b",
    r"\bslate\s+of\s+directors\b",
    r"\bdirector(s)?\s+(skills|qualifications|matrix|independenc(e|y))\b",
    r"\bboard\s+(skills|qualifications)\s+matrix\b",
    r"\bcorporate\s+governance\b",
    r"\bboard\s+committees?\b",
  ],
  "director_bios": [
    r"\b(director|nominee)\s+(biograph(y|ies)|qualifications)\b",
    r"\bour\s+board(’|')?s\s+nominees\b",
    r"\bbiographical\s+information\b",
    r"\bqualifications\s+of\s+(the\s+)?nominees\b",
    r"\belection\s+of\s+directors\b",
  ],
  "director_comp": [
    r"\bdirector\s+compensation\b",
    r"\b(non[-\s]?employee|outside)\s+director\s+compensation\b",
  ],
  "auditor_fees": [
    r"\bprincipal\s+accounting\s+fees\b",
    r"\bindependent\s+registered\s+public\s+accounting\s+firm\b",
    r"\baudit\s+fees\b",
  ],
  "skills_block": [
    r"\brelevant\s+skills\s+and\s+qualifications\b",
    r"\bskills\s+and\s+qualifications\b",
    r"\bkey\s+skills\b",
    r"\brelevant\s+experience\b",
  ],
  "age_anchor": [r"\bage\b\s*[:\-]"],
  "board_recommendation": [
    r"\bboard\s+recommendation\b",
    r"\brecommendation\s+of\s+the\s+board\b",
    r"\brecommendation\b",
  ],
}

# # Regex patterns for proxy statements (DEF 14A) — headings/anchors
# # Use with flags = re.IGNORECASE | re.MULTILINE
# HEADING_PATTERNS = {
#   # -------------------------
#   # Core proposal scaffolding
#   # -------------------------
#   "proposal_header": [
#     r"^\s*(proposal|item|matter|agenda\s*item)\s*(no\.|number)?\s*(?P<num>[ivxlcdm]+|\d+)\s*[:\-\u2013]\s*(?P<title>.+)$",
#     r"^\s*(?P<title>.+?)\s*\(?\s*(proposal|item|matter)\s*(?P<num>[ivxlcdm]+|\d+)\s*\)?\s*$",
#     r"^\s*(matters?\s+to\s+be\s+voted\s+on|items?\s+of\s+business)\s*$",
#     r"^\s*(stockholder|shareholder)\s+proposals?\s*(and|&)?\s*(nominations)?\s*$",
#   ],
#   "board_recommendation": [
#     r"\b(recommendation\s+of\s+the\s+board|board\s+recommendation|the\s+board\s+(recommends|opposes))\b",
#     r"\b(the\s+board\s+(recommends|recommends that you vote|unanimously recommends))\b",
#   ],
#   "vote_requirements": [
#     r"\b(vote\s+required|voting\s+standards?|approval\s+requirements?)\b",
#     r"\b(broker\s+non[-\s]?votes?|abstentions?|quorum|discretionary\s+authority)\b",
#   ],
#   "company_response_to_proposal": [
#     r"\b(company|board)\s+(response|statement|opposition|position)\b",
#   ],
#   "supporting_statement": [
#     r"\b(supporting\s+statement|proponent(?:’|')s\s+statement)\b",
#   ],
#   "proponent_identification": [
#     r"\b(proponent|shareholder|stockholder)\s+(name|contact|address|ownership)\b",
#   ],

#   # -------------------------
#   # Common proposal types
#   # -------------------------
#   "say_on_pay": [
#     r"\b(advisory|non[-\s]?binding)\s+vote\s+on\s+executive\s+compensation\b",
#     r"\b(say[-\s]?on[-\s]?pay)\b",
#     r"\b(approval\s+of\s+named\s+executive\s+officer\s+compensation)\b",
#   ],
#   "say_on_frequency": [
#     r"\b(advisory|non[-\s]?binding)\s+vote\s+on\s+the\s+frequency\b",
#     r"\b(say[-\s]?on[-\s]?pay\s+frequency|vote\s+to\s+determine\s+frequency)\b",
#   ],
#   "auditor_ratification": [
#     r"\b(ratification\s+of\s+(the\s+)?appointment\s+of\s+(\w+|\w+\s+\w+)\s+as\s+(our\s+)?independent\b.*?(auditors?|registered\s+public\s+accounting\s+firm))",
#     r"\b(appoint(ment)?\s+of\s+independent\s+(auditors?|registered\s+public\s+accounting\s+firm))\b",
#   ],
#   "equity_plan_approval": [
#     r"\b(approval|amend(ment|ing))\s+of\s+(the\s+)?(equity|stock|incentive)\s+(compensation\s+)?plan\b",
#     r"\b(increase\s+in\s+shares\s+authorized\s+under\s+(the\s+)?(equity|stock|incentive)\s+plan)\b",
#   ],
#   "charter_bylaw_amendments": [
#     r"\b(amend(ment)?\s+to\s+(certificate\s+of\s+incorporation|charter|by[-\s]?laws?))\b",
#     r"\b(eliminat(ion|e)\s+of\s+supermajority|majority\s+voting\s+standard|declassification\s+of\s+board)\b",
#     r"\b(exculpation\s+of\s+officers|DGCL\s+102\(b\)\(7\))\b",
#     r"\b(increase\s+in\s+authorized\s+(capital\s+)?stock|reverse\s+stock\s+split|forward\s+split)\b",
#     r"\b(change\s+of\s+corporate\s+name|reincorporation|exclusive\s+forum)\b",
#   ],
#   "merger_transactions": [
#     r"\b(approval\s+of\s+(merger|business\s+combination|acquisition|asset\s+sale|spin[-\s]?off))\b",
#   ],
#   "adjournment": [
#     r"\b(proposal\s+to\s+adjourn\s+(the\s+)?meeting)\b",
#   ],
#   "director_election": [
#     r"\b(election\s+of\s+directors|to\s+elect\s+directors|nominees?\s+for\s+election)\b",
#     r"\b(slate\s+of\s+directors|class\s+(i+|1|2|3)\s+directors)\b",
#     r"\b(universal\s+proxy|Rule\s+14a[-\u2011]19)\b",
#   ],
#   "shareholder_special_topics": [
#     r"\b(political\s+(contributions?|spending)|lobbying\s+activities?)\b",
#     r"\b(climate|environmental|sustainability|ESG|human\s+capital|DEI|diversity)\b",
#     r"\b(pay\s+gap|median\s+pay|racial\s+equity\s+audit)\b",
#     r"\b(animal\s+welfare|data\s+privacy|cybersecurity)\b",
#   ],

#   # -------------------------
#   # Executive comp (CD&A etc.)
#   # -------------------------
#   "cda_header": [
#     r"\b(compensation\s+discussion\s+and\s+analysis|CD&A)\b",
#   ],
#   "comp_philosophy_elements": [
#     r"\b(compensation\s+(philosophy|objectives?|principles)|elements\s+of\s+compensation)\b",
#   ],
#   "performance_metrics": [
#     r"\b(performance\s+metrics?|goals?|targets?)\b",
#   ],
#   "ownership_guidelines_clawback": [
#     r"\b(stock\s+ownership\s+guidelines?|clawbacks?|recoupment\s+policy)\b",
#     r"\b(hedging|pledging)\s+(policy|prohibited)\b",
#   ],
#   "grant_timing": [
#     r"\b(grant\s+timing|timing\s+of\s+equity\s+awards|spring[-\s]?loading|bullet[-\s]?dodging)\b",
#   ],
#   "peer_group_consultants": [
#     r"\b(compensation\s+peer\s+group)\b",
#     r"\b(compensation\s+consultant(s)?|independence\s+assessment)\b",
#   ],

#   # -------------------------
#   # Exec comp tables
#   # -------------------------
#   "summary_comp_table": [
#     r"\b(summary\s+compensation\s+table|SCT)\b",
#   ],
#   "grants_plan_based_awards": [
#     r"\b(grants?\s+of\s+plan[-\s]?based\s+awards)\b",
#   ],
#   "outstanding_equity_awards": [
#     r"\b(outstanding\s+equity\s+awards\s+at\s+fiscal\s+year[-\s]?end)\b",
#   ],
#   "option_exercises": [
#     r"\b(option\s+exercises\s+and\s+stock\s+vested)\b",
#   ],
#   "nonqualified_deferred_comp": [
#     r"\b(nonqualified\s+deferred\s+compensation|NQDC)\b",
#   ],
#   "pension_benefits": [
#     r"\b(pension\s+benefits?)\b",
#   ],
#   "pay_vs_performance": [
#     r"\b(pay\s+versus\s+performance|pay[-\s]?vs[-\s]?performance|PvP)\b",
#     r"\b(SEC\s+pay\s+versus\s+performance\s+disclosure)\b",
#   ],
#   "ceo_pay_ratio": [
#     r"\b(CEO\s+pay\s+ratio)\b",
#   ],
#   "comp_committee_reports_misc": [
#     r"\b(compensation\s+committee\s+report)\b",
#     r"\b(compensation\s+committee\s+interlocks\s+and\s+insider\s+participation)\b",
#     r"\b(compensation\s+risk\s+assessment)\b",
#   ],

#   # -------------------------
#   # Directors & governance
#   # -------------------------
#   "directors_roster_or_matrix": [
#     r"\b(board\s+of\s+directors)\b",
#     r"\b(slate\s+of\s+directors)\b",
#     r"\bdirector(s)?\s+(skills|qualifications|matrix|independenc(e|y))\b",
#     r"\b(board\s+(skills|qualifications)\s+matrix)\b",
#     r"\b(corporate\s+governance)\b",
#     r"\b(board\s+committees?)\b",
#     r"\b(meetings\s+and\s+attendance)\b",
#     r"\b(board\s+leadership\s+structure|lead\s+independent\s+director)\b",
#     r"\b(risk\s+oversight)\b",
#     r"\b(code\s+of\s+ethics|governance\s+guidelines)\b",
#     r"\b(communication\s+with\s+the\s+board)\b",
#   ],
#   "director_bios": [
#     r"\b(director|nominee)\s+(biograph(y|ies)|qualifications)\b",
#     r"\bour\s+board(’|')?s\s+nominees\b",
#     r"\bbiographical\s+information\b",
#     r"\bqualifications\s+of\s+(the\s+)?nominees\b",
#     r"\belection\s+of\s+directors\b",
#   ],
#   "director_comp": [
#     r"\bdirector\s+compensation\b",
#     r"\b(non[-\s]?employee|outside)\s+director\s+compensation\b",
#   ],
#   "committee_sections": [
#     r"\b(audit|compensation|nominating\s+and\s+governance|risk|finance)\s+committee\b",
#     r"\b(charters?|responsibilities|report\s+of\s+the\s+audit\s+committee)\b",
#   ],

#   # -------------------------
#   # Ownership & related persons
#   # -------------------------
#   "security_ownership": [
#     r"\b(security\s+ownership\s+of\s+certain\s+beneficial\s+owners\s+and\s+management)\b",
#     r"\b(beneficial\s+ownership\s+table)\b",
#   ],
#   "related_person_transactions": [
#     r"\b(certain\s+relationships\s+and\s+related\s+transactions?|related\s+person\s+transactions?)\b",
#   ],
#   "section_16a_compliance": [
#     r"\b(section\s+16\(a\)\s+beneficial\s+ownership\s+reporting\s+compliance)\b",
#   ],

#   # -------------------------
#   # Auditor & fees
#   # -------------------------
#   "auditor_fees": [
#     r"\b(principal\s+accounting\s+fees)\b",
#     r"\b(independent\s+registered\s+public\s+accounting\s+firm)\b",
#     r"\b(audit\s+fees|audit[-\s]?related\s+fees|tax\s+fees|all\s+other\s+fees)\b",
#     r"\b(pre[-\s]?approval\s+polic(y|ies))\b",
#   ],

#   # -------------------------
#   # Meeting logistics & proxy mechanics
#   # -------------------------
#   "meeting_information": [
#     r"\b(annual\s+meeting\s+of\s+stockholders?|special\s+meeting)\b",
#     r"\b(record\s+date|meeting\s+date|time\s+and\s+place|virtual\s+meeting)\b",
#   ],
#   "voting_procedures": [
#     r"\b(how\s+to\s+vote|voting\s+methods?|internet\s+availability\s+of\s+proxy\s+materials)\b",
#     r"\b(reverse\s+split|householding|notice\s+and\s+access|cost\s+of\s+solicitation)\b",
#     r"\b(revocability\s+of\s+proxies|inspector\s+of\s+elections)\b",
#   ],
#   "advance_notice_universal_proxy": [
#     r"\b(advance\s+notice\s+(bylaws?|requirements))\b",
#     r"\b(universal\s+proxy|Rule\s+14a[-\u2011]?19)\b",
#     r"\b(deadline\s+for\s+stockholder\s+proposals?|14a[-\u2011]?8)\b",
#   ],

#   # -------------------------
#   # Skills / quick anchors / OCR helpers
#   # -------------------------
#   "skills_block": [
#     r"\brelevant\s+skills\s+and\s+qualifications\b",
#     r"\bskills\s+and\s+qualifications\b",
#     r"\bkey\s+skills\b",
#     r"\brelevant\s+experience\b",
#   ],
#   "age_anchor": [
#     r"\bage\b\s*[:\-]",
#   ],

#   # -------------------------
#   # Appendices / supplements
#   # -------------------------
#   "appendix": [
#     r"^\s*appendix\s+[a-z0-9]+\s*[:\-\u2013]",
#     r"\b(appendix|annex|exhibit)\b",
#     r"\b(equity\s+plan\s+appendix|bylaw\s+amendment\s+text|charter\s+text)\b",
#   ],
# }
# ---------------------------
# Helpers
# ---------------------------
def normalize_heading(h: str) -> str:
    h = h.strip().lower().replace("&amp;", "&")
    h = re.sub(r"\s+", " ", h)
    return h.strip(" -:·•|")

def parse_sections_by_markdown(text: str) -> list[dict]:
    header_re = re.compile(r"(?m)^##\s+(.*)$")
    matches = list(header_re.finditer(text))
    sections = []
    for i, m in enumerate(matches):
        header = m.group(1).strip()
        start  = m.end()
        end    = matches[i+1].start() if i+1 < len(matches) else len(text)
        content = text[start:end].strip()
        sections.append({
            "index": i,
            "header_raw": header,
            "header_norm": normalize_heading(header),
            "content": content
        })
    return sections

def compile_category_patterns(pdict: Dict[str, list[str]]) -> Dict[str, list[re.Pattern]]:
    flags = re.IGNORECASE | re.UNICODE | re.MULTILINE
    return {cat: [re.compile(p, flags) for p in plist] for cat, plist in pdict.items()}

def classify_section(section: dict,
                     cat_patterns: Dict[str, list[re.Pattern]],
                     search_content: bool = True,
                     prefer_header: bool = True) -> list[tuple[str, str]]:
    header_text  = section["header_raw"]
    content_text = section["content"]
    hits = []
    for cat, patterns in cat_patterns.items():
        header_hit = any(p.search(header_text) for p in patterns)
        if header_hit:
            hits.append((cat, "header"))
            continue
        if search_content:
            content_hit = any(p.search(content_text) for p in patterns)
            if content_hit:
                hits.append((cat, "content"))
    return hits

def normalize_content_for_hash(txt: str) -> str:
    t = txt.lower().replace("&amp;", "&")
    t = re.sub(r"[^a-z0-9\s]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def content_hash(txt: str) -> str:
    return hashlib.sha1(normalize_content_for_hash(txt).encode("utf-8")).hexdigest()

def remove_near_duplicates(df: pd.DataFrame, text_col: str, sim_threshold: float = 0.95) -> pd.DataFrame:
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
    except Exception:
        print("scikit-learn not available; skipping near-duplicate removal.")
        return df
    texts = df[text_col].tolist()
    vec = TfidfVectorizer(min_df=1, ngram_range=(1,2))
    X = vec.fit_transform(texts)
    keep_idx = []
    kept = []
    for i in range(X.shape[0]):
        if not kept:
            keep_idx.append(i); kept.append(i); continue
        sims = cosine_similarity(X[i], X[keep_idx]).ravel()
        if sims.max() >= sim_threshold:
            continue
        keep_idx.append(i); kept.append(i)
    return df.iloc[keep_idx].reset_index(drop=True)

# --- TABLE / CAPTION / LEGEND / FOOTNOTES ---

# Table header separator: | --- | :---: | ---: |
TABLE_SEP_RE = re.compile(r"^\s*\|?\s*:?-{3,}:?\s*(\|\s*:?-{3,}:?\s*)+\|?\s*$")
def _is_row_like(ln: str) -> bool:
    return ln.count("|") >= 2

def has_markdown_table(content: str) -> bool:
    lines = [ln.rstrip() for ln in content.splitlines()]
    if not lines:
        return False
    if any(TABLE_SEP_RE.match(ln) for ln in lines):
        return True
    # 2+ consecutive row-like lines
    streak = 0
    for ln in lines:
        if _is_row_like(ln):
            streak += 1
            if streak >= 2:
                return True
        else:
            streak = 0
    return False

def extract_all_tables(content: str) -> list[str]:
    """Extract contiguous row-like/table-separator blocks."""
    lines = [ln.rstrip() for ln in content.splitlines()]
    blocks, cur = [], []
    for ln in lines:
        if _is_row_like(ln) or TABLE_SEP_RE.match(ln):
            cur.append(ln)
        else:
            if cur:
                blocks.append("\n".join(cur).strip())
                cur = []
    if cur: blocks.append("\n".join(cur).strip())
    return [b for b in blocks if b.count("|") >= 4]

# Legend lines: ALLCAPS acronym, dash, description (e.g., "ACC - Audit and Compliance Committee")
LEGEND_RE = re.compile(r"^\s*[A-Z]{2,}\s*[-–—]\s+.+$")

def extract_legends_near_tables(content: str, window: int = 12) -> list[str]:
    """Collect lines matching legend pattern within a window around any table."""
    lines = content.splitlines()
    # Mark table line indices
    table_line_idx = set()
    for i, ln in enumerate(lines):
        if _is_row_like(ln) or TABLE_SEP_RE.match(ln):
            table_line_idx.add(i)
    if not table_line_idx:
        return []
    lo = max(0, min(table_line_idx) - window)
    hi = min(len(lines), max(table_line_idx) + window + 1)
    legends = [ln.strip() for ln in lines[lo:hi] if LEGEND_RE.match(ln)]
    # de-dup while preserving order
    seen, out = set(), []
    for x in legends:
        if x not in seen:
            seen.add(x); out.append(x)
    return out

# Captions: narrative lines nearby containing "table"
CAPTION_RE = re.compile(r"\btable\b", re.IGNORECASE)

def extract_caption_near_table(content: str, window: int = 4) -> str:
    lines = content.splitlines()
    # Find first table block start line
    first_tbl = None
    for i, ln in enumerate(lines):
        if _is_row_like(ln) or TABLE_SEP_RE.match(ln):
            first_tbl = i; break
    if first_tbl is None:
        return ""
    lo = max(0, first_tbl - window)
    hi = min(len(lines), first_tbl + 1)
    candidates = [ln.strip() for ln in lines[lo:hi] if CAPTION_RE.search(ln)]
    return candidates[0] if candidates else ""

# Footnotes:
#  Start-of-footnote pattern with optional bullet and broad marker support
FOOTNOTE_START_RE = re.compile(
    r"""^\s*                                   # optional leading spaces
        (?:[-•*]\s*)?                          # optional bullet: -, •, *
        \(\s*(?:\d+|[A-Za-z]+|[IVXLCDM]+)\s*\) # (1) or (a/A) or (iv/IV)
        [\s.:;-]+                               # required delimiter after marker
    """,
    re.VERBOSE
)

def extract_footnote_paragraphs(text: str) -> list[str]:
    """
    Extract footnotes and handle multi-line continuation.
    Stops when hitting another footnote start OR a blank line.
    """
    lines = text.splitlines()
    notes, buf, in_note = [], [], False

    def flush():
        nonlocal buf
        if buf:
            notes.append("\n".join(buf).strip())
            buf = []

    i = 0
    while i < len(lines):
        ln = lines[i]
        if FOOTNOTE_START_RE.match(ln):
            if in_note:
                flush()
            in_note = True
            buf = [ln]
            i += 1
            while i < len(lines):
                nxt = lines[i]
                if not nxt.strip():  # blank line ends the note
                    buf.append(nxt)
                    i += 1
                    break
                if FOOTNOTE_START_RE.match(nxt):  # next footnote
                    break
                buf.append(nxt)
                i += 1
            flush()
            in_note = False
            continue
        i += 1

    return [n for n in notes if n.strip()]

# ---------------------------
# Main
# ---------------------------
if not INPUT_PATH.exists():
    raise FileNotFoundError(f"Input not found: {INPUT_PATH}")

text = INPUT_PATH.read_text(encoding="utf-8", errors="ignore")
sections = parse_sections_by_markdown(text)
sections = [s for s in sections if s["header_norm"] not in STRUCTURAL_HEADINGS]

cat_patterns = compile_category_patterns(HEADING_PATTERNS)

rows = []
for s in sections:
    hits = classify_section(
        s, cat_patterns,
        search_content=SEARCH_CONTENT_TOO,
        prefer_header=PREFER_HEADER_MATCH
    )
    if not hits:
        continue

    tables = extract_all_tables(s["content"])
    is_table = bool(tables)
    legends = extract_legends_near_tables(s["content"]) if is_table else []
    caption = extract_caption_near_table(s["content"]) if is_table else ""
    notes = extract_footnote_paragraphs(s["content"]) if is_table else []

    tables_joined = "\n\n---\n\n".join(tables) if tables else ""
    legends_joined = "\n".join(legends) if legends else ""
    notes_joined   = "\n\n".join(notes) if notes else ""

    for cat, src in hits:
        rows.append({
            "section_index": s["index"],
            "category": cat,
            "header_raw": s["header_raw"],
            "header_norm": s["header_norm"],
            "match_source": src,
            "content": s["content"],
            "has_table": is_table,
            "table_caption": caption,
            "table_markdown": tables_joined,
            "table_legends": legends_joined,
            "table_footnotes": notes_joined
        })

df = pd.DataFrame(rows, columns=[
    "section_index","category","header_raw","header_norm","match_source",
    "content","has_table","table_caption","table_markdown","table_legends","table_footnotes"
])

# --- Deduplicate by CONTENT (exact) ---
if not df.empty:
    df["content_fp"] = df["content"].map(content_hash)
    df = df.sort_values("section_index").drop_duplicates(subset=["content_fp"], keep="first")
    if USE_FUZZY_NEAR_DUPES:
        df = remove_near_duplicates(df.reset_index(drop=True), text_col="content", sim_threshold=FUZZY_SIM_THRESHOLD)

# --- Group non-table first, then table sections (with a separator) ---
if not df.empty:
    non_table = df[df["has_table"] == False].copy().sort_values("section_index")
    table     = df[df["has_table"] == True].copy().sort_values("section_index")
    non_table.insert(0, "group", "text")
    table.insert(0, "group", "table")

    separator = pd.DataFrame([{
        "group": "separator",
        "section_index": "",
        "category": "",
        "header_raw": "=== TABLE SECTIONS BELOW ===",
        "header_norm": "",
        "match_source": "",
        "content": "",
        "has_table": "",
        "table_caption": "",
        "table_markdown": "",
        "table_legends": "",
        "table_footnotes": "",
        "content_fp": ""
    }])

    out_df = pd.concat([non_table, separator, table], ignore_index=True)
    out_df = out_df.drop(columns=["content_fp"], errors="ignore")
else:
    out_df = df.drop(columns=["content_fp"], errors="ignore")

# Save CSV
out_df.to_csv(OUT_CSV, index=False, quoting=csv.QUOTE_MINIMAL)
print(f"Saved grouped CSV to: {OUT_CSV}")
try:
    from IPython.display import display
    display(out_df.tail(12))  # show tail to visualize table group end
except Exception:
    pass

"""#version 2"""

# Colab-ready: UNIVERSAL table extractor (no heading-pattern anchors).
# Scans the entire Docling TXT for markdown-style tables (| ... |),
# harvests caption/context above, legends, and footnotes below,
# and exports a deduped CSV.
#
# Output CSV columns:
#   group, section_index, header_raw, header_norm, content,
#   has_table, table_caption, table_markdown, table_legends, table_footnotes,
#   table_context_above, table_first_header_cells
#
# Notes:
# - No HEADING_PATTERNS or category classification.
# - Sections are split by lines that start with "## " (if present). If none,
#   the entire file is treated as one section.
# - Deduplication is by a SHA-1 fingerprint of the "content" field.
# - Optional near-duplicate removal via TF-IDF cosine similarity (toggle).

import re, csv, hashlib
import pandas as pd
from pathlib import Path
from typing import List, Dict

# ---------------------------
# Config
# ---------------------------
# If running in Colab, put your Docling-exported TXT in /content/...
# If running here with the uploaded asset, default to the provided /mnt/data path.
INPUT_PATH = Path("/mnt/data/goog_proxy_1_custom_output.txt")  # fallback works in this chat
if not INPUT_PATH.exists():
    INPUT_PATH = Path("/content/goog_proxy_1_custom_output.txt")  # <-- change if needed

OUT_CSV = Path("/content/universal_tables_dedup_by_content.csv")

# Treat purely structural nav headings as ignorable section headers (optional)
STRUCTURAL_HEADINGS = {
    "back to contents",
    "table of contents",
    "alphabet 2025 proxy statement",
}

# Near-duplicate (fuzzy) removal: requires scikit-learn (optional)
USE_FUZZY_NEAR_DUPES = False
FUZZY_SIM_THRESHOLD = 0.95  # cosine similarity threshold (0..1), higher = stricter

# Context windows
CAPTION_WINDOW_ABOVE = 4
LEGEND_WINDOW = 12

# ---------------------------
# Helpers: normalization & sectioning
# ---------------------------
def normalize_heading(h: str) -> str:
    h = h.strip().lower().replace("&amp;", "&")
    h = re.sub(r"\s+", " ", h)
    return h.strip(" -:·•|")

def parse_sections_by_markdown(text: str) -> list[dict]:
    """
    Split the text into sections by '## <Header>' markers.
    If no headers are found, return a single section covering the full text.
    """
    header_re = re.compile(r"(?m)^##\s+(.*)$")
    matches = list(header_re.finditer(text))
    sections = []

    if not matches:
        sections.append({
            "index": 0,
            "header_raw": "(entire document)",
            "header_norm": "(entire document)",
            "content": text.strip()
        })
        return sections

    for i, m in enumerate(matches):
        header = m.group(1).strip()
        start  = m.end()
        end    = matches[i+1].start() if i+1 < len(matches) else len(text)
        content = text[start:end].strip()
        sections.append({
            "index": i,
            "header_raw": header,
            "header_norm": normalize_heading(header),
            "content": content
        })
    return sections

# ---------------------------
# Dedup helpers
# ---------------------------
def normalize_content_for_hash(txt: str) -> str:
    t = txt.lower().replace("&amp;", "&")
    t = re.sub(r"[^a-z0-9\s]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def content_hash(txt: str) -> str:
    return hashlib.sha1(normalize_content_for_hash(txt).encode("utf-8")).hexdigest()

def remove_near_duplicates(df: pd.DataFrame, text_col: str, sim_threshold: float = 0.95) -> pd.DataFrame:
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
    except Exception:
        print("scikit-learn not available; skipping near-duplicate removal.")
        return df
    texts = df[text_col].tolist()
    vec = TfidfVectorizer(min_df=1, ngram_range=(1,2))
    X = vec.fit_transform(texts)
    keep_idx = []
    kept = []
    for i in range(X.shape[0]):
        if not kept:
            keep_idx.append(i); kept.append(i); continue
        sims = cosine_similarity(X[i], X[keep_idx]).ravel()
        if sims.max() >= sim_threshold:
            continue
        keep_idx.append(i); kept.append(i)
    return df.iloc[keep_idx].reset_index(drop=True)

# ---------------------------
# Table/Caption/Legend/Footnote detectors (universal)
# ---------------------------
ROW_LIKE_MIN_PIPES = 2
TABLE_SEP_RE = re.compile(r"^\s*\|?\s*:?-{3,}:?\s*(\|\s*:?-{3,}:?\s*)+\|?\s*$")
HEADER_LINE_RE = re.compile(r"^\s*##\s+")  # in case a stray header leaked into content

def _is_row_like(ln: str) -> bool:
    # Consider as row-like if it has multiple pipes AND at least some non-pipe content
    return (ln.count("|") >= ROW_LIKE_MIN_PIPES) and (re.sub(r"[|\s:-]", "", ln) != "")

def _is_tableish_line(ln: str) -> bool:
    ln = ln.rstrip()
    return _is_row_like(ln) or bool(TABLE_SEP_RE.match(ln))

CAPTION_RE = re.compile(r"\btable\b", re.IGNORECASE)
LEGEND_RE  = re.compile(r"^\s*[A-Z]{2,}\s*[-–—]\s+.+$")

# Broad footnote pattern: bullets optional, (1)/(a)/(IV)-style markers.
FOOTNOTE_START_RE = re.compile(
    r"""^\s*                                   # optional leading spaces
        (?:[-•*]\s*)?                          # optional bullet: -, •, *
        \(\s*(?:\d+|[A-Za-z]+|[IVXLCDM]+)\s*\) # (1) or (a/A) or (iv/IV)
        [\s.:;-]+                               # delimiter after marker
    """,
    re.VERBOSE
)

def _grab_context_above(lines: List[str], start_idx: int, window: int = CAPTION_WINDOW_ABOVE) -> str:
    """
    Pull a short narrative from up to `window` lines above the first table line.
    Prefers a single line with 'table' in it; else returns the closest non-empty sentence-like line.
    """
    lo = max(0, start_idx - window)
    candidates = []
    for j in range(start_idx-1, lo-1, -1):
        cand = lines[j].strip()
        if not cand or HEADER_LINE_RE.match(cand):
            continue
        candidates.append(cand)
        if CAPTION_RE.search(cand):
            return cand  # strong signal for caption
    return candidates[0] if candidates else ""

def _first_header_cells(table_md: str, max_cells: int = 6) -> str:
    """
    Heuristic: return the first non-separator row's first few cells,
    useful for downstream classification/dedup.
    """
    for ln in table_md.splitlines():
        if TABLE_SEP_RE.match(ln.strip()):
            continue
        if ln.count("|") >= ROW_LIKE_MIN_PIPES:
            cells = [c.strip() for c in ln.strip().strip("|").split("|")]
            cells = [c for c in cells if c]
            if cells:
                return ", ".join(cells[:max_cells])
    return ""

def find_tables_with_context(section_text: str,
                             caption_window_above: int = CAPTION_WINDOW_ABOVE,
                             legend_window: int = LEGEND_WINDOW,
                             include_legends: bool = True) -> list[dict]:
    """
    Return a list of tables with nearby caption, legends, footnotes, and context-above.
    Each item: { 'table_markdown', 'caption', 'context_above', 'legends', 'footnotes', 'first_header_cells' }.
    """
    lines = section_text.splitlines()
    n = len(lines)
    tables = []

    i = 0
    while i < n:
        # Seek the start of a table block
        if not _is_tableish_line(lines[i]):
            i += 1
            continue

        # Accumulate contiguous table-ish block, allowing 1 grace non-table line
        start = i
        grace = 1
        block = [lines[i].rstrip()]
        i += 1
        while i < n:
            ln = lines[i]
            if _is_tableish_line(ln):
                block.append(ln.rstrip()); grace = 1; i += 1; continue
            # tolerate one noisy line (OCR artifacts) between rows
            if grace > 0 and ln.strip() != "" and not HEADER_LINE_RE.match(ln):
                block.append(ln.rstrip()); grace -= 1; i += 1; continue
            break
        end = i  # first line after the block

        # Build table markdown & validate
        table_md = "\n".join(block).strip()
        if table_md.count("|") < 4:
            # too flimsy to be a real table
            continue

        # Caption and narrative context above
        caption = ""
        context_above = _grab_context_above(lines, start, caption_window_above)
        if CAPTION_RE.search(context_above):
            caption = context_above  # treat as caption if it mentions "table"

        # Legends near the table (around window)
        legends = []
        if include_legends:
            seen = set()
            loL = max(0, start - legend_window)
            hiL = min(n, end + legend_window)
            for j in range(loL, hiL):
                cand = lines[j].strip()
                if LEGEND_RE.match(cand) and cand not in seen:
                    seen.add(cand)
                    legends.append(cand)

        # Footnotes below: walk down until blank/next footnote/header/non-footnote paragraph
        notes = []
        k = end
        while k < n:
            ln = lines[k]
            if HEADER_LINE_RE.match(ln):  # new section (just in case)
                break
            if not ln.strip():
                k += 1
                continue
            if FOOTNOTE_START_RE.match(ln):
                buf = [ln.rstrip()]
                k += 1
                while k < n:
                    nxt = lines[k]
                    if HEADER_LINE_RE.match(nxt) or FOOTNOTE_START_RE.match(nxt) or nxt.strip() == "":
                        break
                    buf.append(nxt.rstrip())
                    k += 1
                notes.append("\n".join(buf).strip())
                continue
            # stop scanning when regular paragraph encountered
            break

        tables.append({
            "table_markdown": table_md,
            "caption": caption,
            "context_above": context_above if not caption else "",  # avoid duplicating caption
            "legends": "\n".join(legends) if legends else "",
            "footnotes": "\n\n".join(notes) if notes else "",
            "first_header_cells": _first_header_cells(table_md),
        })

    return tables

# ---------------------------
# Main
# ---------------------------
if not INPUT_PATH.exists():
    raise FileNotFoundError(f"Input not found: {INPUT_PATH}")

text = INPUT_PATH.read_text(encoding="utf-8", errors="ignore")

# Parse sections by '##', filter out purely structural headings (optional)
sections = parse_sections_by_markdown(text)
sections = [s for s in sections if normalize_heading(s["header_raw"]) not in STRUCTURAL_HEADINGS]

rows = []
for s in sections:
    tables_info = find_tables_with_context(s["content"])
    is_table = len(tables_info) > 0

    if is_table:
        for ti in tables_info:
            rows.append({
                "section_index": s["index"],
                "header_raw": s["header_raw"],
                "header_norm": s["header_norm"],
                "content": s["content"],
                "has_table": True,
                "table_caption": ti["caption"],
                "table_markdown": ti["table_markdown"],
                "table_legends": ti["legends"],
                "table_footnotes": ti["footnotes"],
                "table_context_above": ti["context_above"],
                "table_first_header_cells": ti["first_header_cells"],
            })
    else:
        # Keep non-table sections too (useful for context or auditing)
        rows.append({
            "section_index": s["index"],
            "header_raw": s["header_raw"],
            "header_norm": s["header_norm"],
            "content": s["content"],
            "has_table": False,
            "table_caption": "",
            "table_markdown": "",
            "table_legends": "",
            "table_footnotes": "",
            "table_context_above": "",
            "table_first_header_cells": "",
        })

# Build DataFrame
df_cols = [
    "section_index","header_raw","header_norm","content",
    "has_table","table_caption","table_markdown","table_legends","table_footnotes",
    "table_context_above","table_first_header_cells"
]
df = pd.DataFrame(rows, columns=df_cols)

# --- Deduplicate by CONTENT (exact) ---
if not df.empty:
    df["content_fp"] = df["content"].map(content_hash)
    df = df.sort_values("section_index").drop_duplicates(subset=["content_fp"], keep="first")
    if USE_FUZZY_NEAR_DUPES:
        df = remove_near_duplicates(df.reset_index(drop=True), text_col="content", sim_threshold=FUZZY_SIM_THRESHOLD)

# --- Group non-table first, then table sections (with a separator) ---
if not df.empty:
    non_table = df[df["has_table"] == False].copy().sort_values("section_index")
    table     = df[df["has_table"] == True].copy().sort_values("section_index")
    non_table.insert(0, "group", "text")
    table.insert(0, "group", "table")

    separator = pd.DataFrame([{
        "group": "separator",
        "section_index": "",
        "header_raw": "=== TABLE SECTIONS BELOW ===",
        "header_norm": "",
        "content": "",
        "has_table": "",
        "table_caption": "",
        "table_markdown": "",
        "table_legends": "",
        "table_footnotes": "",
        "table_context_above": "",
        "table_first_header_cells": "",
        "content_fp": ""
    }])

    out_df = pd.concat([non_table, separator, table], ignore_index=True)
    out_df = out_df.drop(columns=["content_fp"], errors="ignore")
else:
    out_df = df.drop(columns=["content_fp"], errors="ignore")

# Save CSV
out_df.to_csv(OUT_CSV, index=False, quoting=csv.QUOTE_MINIMAL)
print(f"Saved universal CSV to: {OUT_CSV}")

# Quick peek
try:
    from IPython.display import display
    display(out_df.head(8))
    display(out_df.tail(8))
except Exception:
    pass

"""#Version 3 with table merging"""

# Colab-ready: UNIVERSAL table extractor + Strategy B + Row-wise Merge
# + Section-end Footnotes + Multi-stitch + Tables-only CSV (dedup by table markdown, legends/footnotes preserved)
# + Robust merge handling duplicate column names
# ---------------------------------------------------------------------------------
# IN : meta_proxy_1_docling_output.txt  (current directory)
# OUT: docling_text_cleaned_strategyB.txt
#      universal_tables_dedup_by_content.csv         (sections + raw tables; ALL tables kept)
#      universal_tables_merged_by_headers.csv        (merged tables; robust to dup headers)
#      universal_tables_tables_only.csv              (tables + captions/legends/footnotes; DEDUPED)
#
import re, csv, hashlib
import pandas as pd
from pathlib import Path
from typing import List, Tuple
from collections import Counter

# ---------------------------
# CONFIG (paths: current directory only)
# ---------------------------
INPUT_PATH = Path("goog_proxy_1_docling_output.txt")
if not INPUT_PATH.exists():
    raise FileNotFoundError("Expected 'goog_proxy_1_docling_output.txt' in the current directory.")

OUT_CSV_SECTIONS = Path("universal_tables_dedup_by_content.csv")
OUT_CSV_MERGED   = Path("universal_tables_merged_by_headers.csv")
OUT_CSV_TABLES   = Path("universal_tables_tables_only.csv")
CLEAN_TXT_OUT    = Path("docling_text_cleaned_strategyB.txt")

STRUCTURAL_HEADINGS = {
    "back to contents",
    "table of contents",
    "alphabet 2025 proxy statement",
}

# Strategy B cleaner
FREQ_THRESHOLD    = 0.006
REGEX_BLOCKLIST = [
    r"^\s*back\s+to\s+contents\s*#*\s*$",
    r"^\s*table\s+of\s+contents\s*$",
    r"^\s*alphabet\s+20\d{2}\s+proxy\s+statement\s*(?:\d+)?\s*$",
    r"^\s*(corporate|proxy)\s+statement\s+summary\s*&\s*highlights\s*$",
    r"^\s*corporate\s+governance\s*$",
    r"^\s*director(s)?\s+and\s+executive\s+compensation\s*$",
    r"^\s*audit\s+matters\s*$",
    r"^\s*proposals\s*$",
    r"^\s*q\s*&\s*a\s*$",
    r"^\s*page\s*\d+\s*$",
    r"^\s*\d+\s*$",
]

# Extraction healing
CAPTION_WINDOW_ABOVE = 4
LEGEND_WINDOW        = 12
ALLOW_TABLE_GAP      = 3   # tolerated junk lines inside a table block
ALLOW_STITCH_GAP     = 3   # tolerated ignorable lines between table blocks to stitch them

# Merging by headers
MERGE_HEADER_SIM_THRESHOLD = 0.90  # 0..1, Jaccard over normalized header tokens
MERGE_MAX_LINE_GAP         = 60    # max lines between two tables to consider merging

# Optional near-duplicate removal for section bodies (disabled by default)
USE_FUZZY_NEAR_DUPES = False
FUZZY_SIM_THRESHOLD  = 0.95

# ---------------------------
# Helpers: normalization & sectioning
# ---------------------------
def normalize_heading(h: str) -> str:
    h = h.strip().lower().replace("&amp;", "&")
    h = re.sub(r"\s+", " ", h)
    return h.strip(" -:·•|")

def parse_sections_by_markdown(text: str) -> List[dict]:
    header_re = re.compile(r"(?m)^##\s+(.*)$")
    matches = list(header_re.finditer(text))
    sections = []
    if not matches:
        sections.append({
            "index": 0,
            "header_raw": "(entire document)",
            "header_norm": "(entire document)",
            "content": text.strip(),
            "span": (0, len(text))
        })
        return sections
    for i, m in enumerate(matches):
        header = m.group(1).strip()
        start  = m.end()
        end    = matches[i+1].start() if i+1 < len(matches) else len(text)
        content = text[start:end].strip()
        sections.append({
            "index": i,
            "header_raw": header,
            "header_norm": normalize_heading(header),
            "content": content,
            "span": (start, end)
        })
    return sections

# ---------------------------
# Strategy B — boilerplate cleaner
# ---------------------------
def _norm_for_freq(s: str) -> str:
    s2 = re.sub(r"\s+", " ", s).strip().lower()
    s2 = re.sub(r"\bpage\s*\d+\b", " ", s2)
    s2 = re.sub(r"\b\d{1,4}\b", " ", s2)
    s2 = re.sub(r"\s+", " ", s2).strip()
    return s2

def _build_line_frequency(lines: List[str]) -> Counter:
    cnt = Counter()
    for ln in lines:
        z = _norm_for_freq(ln)
        if len(z) >= 3:
            cnt[z] += 1
    return cnt

def _is_blocklisted(ln: str) -> bool:
    for rx in REGEX_BLOCKLIST:
        if re.search(rx, ln, flags=re.IGNORECASE):
            return True
    return False

def _is_boilerplate(ln: str, freq: Counter, total: int, freq_threshold: float) -> bool:
    if not ln.strip():
        return False
    norm = _norm_for_freq(ln)
    if not norm:
        return False
    if freq.get(norm, 0) / max(total, 1) >= freq_threshold:
        return True
    if _is_blocklisted(ln):
        return True
    return False

def clean_docling_text_strategyB(text: str,
                                 freq_threshold: float = FREQ_THRESHOLD) -> str:
    lines = text.splitlines()
    freq = _build_line_frequency(lines)
    total = len(lines)
    cleaned = [ln for ln in lines if not _is_boilerplate(ln, freq, total, freq_threshold)]
    return "\n".join(cleaned)

# ---------------------------
# Dedup helpers
# ---------------------------
def normalize_content_for_hash(txt: str) -> str:
    t = txt.lower().replace("&amp;", "&")
    t = re.sub(r"[^a-z0-9\s]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def content_hash(txt: str) -> str:
    return hashlib.sha1(normalize_content_for_hash(txt).encode("utf-8")).hexdigest()

# ---------------------------
# Table detectors + healing
# ---------------------------
ROW_LIKE_MIN_PIPES = 2
TABLE_SEP_RE = re.compile(r"^\s*\|?\s*:?-{3,}:?\s*(\|\s*:?-{3,}:?\s*)+\|?\s*$")
HEADER_LINE_RE = re.compile(r"^\s*##\s+")

def _is_row_like(ln: str) -> bool:
    return (ln.count("|") >= ROW_LIKE_MIN_PIPES) and (re.sub(r"[|\s:-]", "", ln) != "")

def _is_tableish_line(ln: str) -> bool:
    ln = ln.rstrip()
    return _is_row_like(ln) or bool(TABLE_SEP_RE.match(ln))

CAPTION_RE = re.compile(r"\btable\b", re.IGNORECASE)
LEGEND_RE  = re.compile(r"^\s*[A-Z]{2,}\s*[-–—]\s+.+$")
FOOTNOTE_START_RE = re.compile(
    r"""^\s*
        (?:[-•*]\s*)?
        \(\s*(?:\d+|[A-Za-z]+|[IVXLCDM]+)\s*\)
        [\s.:;-]+
    """, re.VERBOSE
)

def _grab_context_above(lines: List[str], start_idx: int, window: int = CAPTION_WINDOW_ABOVE) -> str:
    lo = max(0, start_idx - window)
    candidates = []
    for j in range(start_idx-1, lo-1, -1):
        cand = lines[j].strip()
        if not cand or HEADER_LINE_RE.match(cand):
            continue
        candidates.append(cand)
        if CAPTION_RE.search(cand):
            return cand
    return candidates[0] if candidates else ""

def _first_header_cells(table_md: str, max_cells: int = 6) -> str:
    for ln in table_md.splitlines():
        if TABLE_SEP_RE.match(ln.strip()):
            continue
        if ln.count("|") >= ROW_LIKE_MIN_PIPES:
            cells = [c.strip() for c in ln.strip().strip("|").split("|")]
            cells = [c for c in cells if c]
            if cells:
                return ", ".join(cells[:max_cells])
    return ""

def _is_ignorable_inline_junk(ln: str) -> bool:
    if not ln.strip():
        return True
    if _is_blocklisted(ln):
        return True
    if re.search(r"\b(?:page\s*)?\d{1,4}\b", ln.strip().lower()):
        if ln.count("|") < 2:
            return True
    return False

def _harvest_table_block(lines: List[str], start_idx: int) -> tuple[int, str]:
    n = len(lines)
    i = start_idx
    block = [lines[i].rstrip()]
    i += 1
    grace = ALLOW_TABLE_GAP
    while i < n:
        ln = lines[i]
        if _is_tableish_line(ln):
            block.append(ln.rstrip()); grace = ALLOW_TABLE_GAP; i += 1; continue
        if grace > 0 and _is_ignorable_inline_junk(ln) and not HEADER_LINE_RE.match(ln):
            grace -= 1; i += 1; continue
        break
    return i, "\n".join(block).strip()

def _maybe_stitch_tables(lines: List[str], end_idx: int, current_table_md: str) -> tuple[int, str]:
    """
    Keep stitching subsequent table blocks separated by up to ALLOW_STITCH_GAP ignorable lines.
    """
    n = len(lines)
    total_end = end_idx
    stitched_md = current_table_md
    while True:
        gap = 0
        j = total_end
        found_next = False
        while j < n and gap <= ALLOW_STITCH_GAP:
            if _is_tableish_line(lines[j]):
                next_end, next_md = _harvest_table_block(lines, j)
                stitched_md = stitched_md + "\n" + next_md
                total_end = next_end
                found_next = True
                break
            if _is_ignorable_inline_junk(lines[j]):
                gap += 1; j += 1; continue
            break
        if not found_next:
            break
    return total_end, stitched_md

def find_tables_with_context(section_text: str,
                             caption_window_above: int = CAPTION_WINDOW_ABOVE,
                             legend_window: int = LEGEND_WINDOW,
                             include_legends: bool = True) -> List[dict]:
    """
    Detect all tables in the section; for each, collect footnotes that follow it
    until the NEXT table begins or the SECTION ends.
    """
    lines = section_text.splitlines()
    n = len(lines)
    tables = []
    i = 0
    while i < n:
        if not _is_tableish_line(lines[i]):
            i += 1
            continue

        start = i
        end, table_md = _harvest_table_block(lines, i)
        end2, table_md2 = _maybe_stitch_tables(lines, end, table_md)
        if end2 != end:
            end = end2
            table_md = table_md2

        if table_md.count("|") < 4:
            i = max(end, i + 1)
            continue

        caption = ""
        context_above = _grab_context_above(lines, start, caption_window_above)
        if CAPTION_RE.search(context_above):
            caption = context_above

        # legends near the table
        legends = []
        if include_legends:
            seen = set()
            loL = max(0, start - legend_window)
            hiL = min(n, end + legend_window)
            for j in range(loL, hiL):
                cand = lines[j].strip()
                if LEGEND_RE.match(cand) and cand not in seen:
                    seen.add(cand); legends.append(cand)

        # footnotes until next table or section end
        notes = []
        k = end
        foot_buf = []
        while k < n:
            ln = lines[k]
            if _is_tableish_line(ln) or HEADER_LINE_RE.match(ln):
                break
            if not ln.strip():
                if foot_buf:
                    notes.append("\n".join(foot_buf).strip()); foot_buf = []
                k += 1; continue
            if FOOTNOTE_START_RE.match(ln):
                foot_buf.append(ln.rstrip()); k += 1
                while k < n:
                    nxt = lines[k]
                    if (HEADER_LINE_RE.match(nxt) or _is_tableish_line(nxt)
                        or FOOTNOTE_START_RE.match(nxt) or nxt.strip() == ""):
                        break
                    foot_buf.append(nxt.rstrip()); k += 1
                continue
            k += 1
        if foot_buf:
            notes.append("\n".join(foot_buf).strip())

        tables.append({
            "table_markdown": table_md,
            "caption": caption,
            "context_above": context_above if not caption else "",
            "legends": "\n".join(legends) if legends else "",
            "footnotes": "\n\n".join(notes) if notes else "",
            "first_header_cells": _first_header_cells(table_md),
            "line_start": start,
            "line_end": end
        })

        i = max(end, i + 1)
    return tables

# ---------------------------
# Markdown parsing helpers
# ---------------------------
def _normalize_col_name(s: str) -> str:
    s = s.lower().replace("&amp;", "&")
    s = re.sub(r"[^a-z0-9&/()+\- ]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _tokenize_header(s: str) -> set:
    s = _normalize_col_name(s)
    toks = re.split(r"[ /()+\-]+", s)
    return {t for t in toks if t}

def _header_signature(cols: List[str]) -> Tuple[str, ...]:
    return tuple(_normalize_col_name(c) for c in cols)

def _header_similarity(a_cols: List[str], b_cols: List[str]) -> float:
    a_tokens = set().union(*[_tokenize_header(c) for c in a_cols]) if a_cols else set()
    b_tokens = set().union(*[_tokenize_header(c) for c in b_cols]) if b_cols else set()
    if not a_tokens and not b_tokens:
        return 1.0
    if not a_tokens or not b_tokens:
        return 0.0
    inter = len(a_tokens & b_tokens)
    union = len(a_tokens | b_tokens)
    return inter / max(union, 1)

def _parse_markdown_table(md: str) -> Tuple[pd.DataFrame, List[str]]:
    lines = [ln for ln in md.splitlines() if ln.strip()]
    header_idx = None
    for idx, ln in enumerate(lines):
        if ln.count("|") >= ROW_LIKE_MIN_PIPES and not TABLE_SEP_RE.match(ln.strip()):
            header_idx = idx; break
    if header_idx is None:
        return pd.DataFrame(), []

    def parse_row(ln: str) -> List[str]:
        raw = ln.strip()
        if raw.startswith("|"): raw = raw[1:]
        if raw.endswith("|"):   raw = raw[:-1]
        return [c.strip() for c in raw.split("|")]

    header = parse_row(lines[header_idx])
    data_start = header_idx + 1
    if data_start < len(lines) and TABLE_SEP_RE.match(lines[data_start].strip()):
        data_start += 1

    rows = []
    for ln in lines[data_start:]:
        if TABLE_SEP_RE.match(ln.strip()):
            continue
        if ln.count("|") < ROW_LIKE_MIN_PIPES:
            break
        row = parse_row(ln)
        if len(row) < len(header):   row = row + [""] * (len(header) - len(row))
        elif len(row) > len(header): row = row[:len(header)]
        rows.append(row)

    df = pd.DataFrame(rows, columns=header)
    return df, header

def _dataframe_to_markdown(df: pd.DataFrame) -> str:
    if df.empty: return ""
    cols = list(df.columns)
    head = "| " + " | ".join(cols) + " |"
    sep  = "| " + " | ".join(["---"] * len(cols)) + " |"
    body = "\n".join("| " + " | ".join("" if pd.isna(v) else str(v) for v in row) + " |"
                     for row in df.fillna("").astype(str).values.tolist())
    return "\n".join([head, sep, body])

# ---------------------------
# NEW: Make df columns unique (prevents InvalidIndexError during concat/reindex)
# ---------------------------
def _make_unique_columns(cols: List[str]) -> List[str]:
    """Return a list of column names made unique by suffixing duplicates."""
    counts = {}
    out = []
    for c in cols:
        if c in counts:
            counts[c] += 1
            out.append(f"{c} [{counts[c]}]")  # e.g., "2024 [1]"
        else:
            counts[c] = 0
            out.append(c)
    return out

def _ensure_unique_df_columns(df: pd.DataFrame) -> pd.DataFrame:
    if not isinstance(df, pd.DataFrame) or df.empty:
        return df
    new_cols = _make_unique_columns(list(df.columns))
    if len(new_cols) != len(df.columns) or any(a != b for a, b in zip(new_cols, df.columns)):
        df = df.copy()
        df.columns = new_cols
    return df

# ---------------------------
# Merge (robust to duplicate columns)
# ---------------------------
def merge_tables_by_headers(tables: List[dict],
                            header_sim_threshold: float = MERGE_HEADER_SIM_THRESHOLD,
                            max_line_gap: int = MERGE_MAX_LINE_GAP) -> List[dict]:
    """
    Merge adjacent/nearby tables when headers are similar and close in the text.
    Robust to duplicate column names by uniquifying columns and aligning via union.
    """
    enriched = []
    for t in tables:
        df, hdr = _parse_markdown_table(t["table_markdown"])
        if df.empty:
            continue
        # Ensure columns are unique before any alignment/concat
        df = _ensure_unique_df_columns(df)
        # Keep headers in sync with the uniquified df
        hdr = list(df.columns)

        t2 = dict(t)
        t2["df"] = df
        t2["hdr"] = hdr
        enriched.append(t2)

    if not enriched:
        return []

    # Sort by source position
    enriched.sort(key=lambda x: x["line_start"])

    merged = []
    cur = enriched[0]
    for nxt in enriched[1:]:
        # Header similarity is computed on token sets (after normalization)
        hdr_sim = _header_similarity(list(cur["hdr"]), list(nxt["hdr"]))
        gap = nxt["line_start"] - cur["line_end"]
        can_merge = (hdr_sim >= header_sim_threshold) and (0 <= gap <= max_line_gap)

        if can_merge:
            # Re-uniquify (paranoia)
            cur["df"] = _ensure_unique_df_columns(cur["df"])
            nxt["df"] = _ensure_unique_df_columns(nxt["df"])

            # Align by UNION of column names (order: current -> add any new from next)
            all_cols = list(dict.fromkeys(list(cur["df"].columns) + list(nxt["df"].columns)))
            cur_df_aligned = cur["df"].reindex(columns=all_cols)
            nxt_df_aligned = nxt["df"].reindex(columns=all_cols)

            # Concatenate rows
            cur["df"] = pd.concat([cur_df_aligned, nxt_df_aligned], ignore_index=True)

            # Rebuild markdown & update bounds/metadata
            cur["table_markdown"] = _dataframe_to_markdown(cur["df"])
            cur["hdr"] = list(cur["df"].columns)
            cur["line_end"] = max(cur["line_end"], nxt["line_end"])

            def _concat_unique(a: str, b: str) -> str:
                parts = [p for p in (a, b) if p and p.strip()]
                seen, out = set(), []
                for p in parts:
                    if p not in seen:
                        seen.add(p); out.append(p)
                return "\n\n".join(out)

            cur["caption"]    = _concat_unique(cur.get("caption",""), nxt.get("caption",""))
            cur["legends"]    = _concat_unique(cur.get("legends",""), nxt.get("legends",""))
            cur["footnotes"]  = _concat_unique(cur.get("footnotes",""), nxt.get("footnotes",""))
            cur["first_header_cells"] = _first_header_cells(cur["table_markdown"])
        else:
            merged.append(cur)
            cur = nxt

    merged.append(cur)

    # Drop helper fields before returning
    for m in merged:
        for k in ["df", "hdr"]:
            m.pop(k, None)

    return merged

# ---------------------------
# MAIN
# ---------------------------
raw_text = INPUT_PATH.read_text(encoding="utf-8", errors="ignore")

# 1) Clean text
cleaned_text = clean_docling_text_strategyB(raw_text, freq_threshold=FREQ_THRESHOLD)
CLEAN_TXT_OUT.write_text(cleaned_text, encoding="utf-8")
print(f"Saved cleaned text to: {CLEAN_TXT_OUT}")

# 2) Section parsing
sections = parse_sections_by_markdown(cleaned_text)
sections = [s for s in sections if normalize_heading(s["header_raw"]) not in STRUCTURAL_HEADINGS]

# 3) Extract tables per section
rows = []
tables_per_section = []  # for merging per section and for tables-only export
for s in sections:
    tables_info = find_tables_with_context(s["content"])
    tables_per_section.append((s, tables_info))

    if tables_info:
        for ti in tables_info:
            rows.append({
                "section_index": s["index"],
                "header_raw": s["header_raw"],
                "header_norm": s["header_norm"],
                "content": s["content"],
                "has_table": True,
                "table_caption": ti["caption"],
                "table_markdown": ti["table_markdown"],
                "table_legends": ti["legends"],
                "table_footnotes": ti["footnotes"],
                "table_context_above": ti["context_above"],
                "table_first_header_cells": ti["first_header_cells"],
            })
    else:
        rows.append({
            "section_index": s["index"],
            "header_raw": s["header_raw"],
            "header_norm": s["header_norm"],
            "content": s["content"],
            "has_table": False,
            "table_caption": "",
            "table_markdown": "",
            "table_legends": "",
            "table_footnotes": "",
            "table_context_above": "",
            "table_first_header_cells": "",
        })

# 4) Build sections CSV (raw tables) — FIXED DEDUPE (preserve multiple tables per section)
df_cols = [
    "section_index","header_raw","header_norm","content",
    "has_table","table_caption","table_markdown","table_legends","table_footnotes",
    "table_context_above","table_first_header_cells"
]
df = pd.DataFrame(rows, columns=df_cols)

if not df.empty:
    df["content_fp"] = df["content"].map(content_hash)
    df["table_fp"] = df["table_markdown"].map(lambda x: content_hash(x) if isinstance(x, str) and x.strip() else "")
    df = (df
          .sort_values(["section_index"])
          .drop_duplicates(subset=["content_fp","table_fp","has_table"], keep="first"))
    if USE_FUZZY_NEAR_DUPES:
        # Optional near-duplicate removal at the section level (doesn't collapse multiple tables)
        sec_df = df.drop_duplicates(subset=["content_fp"]).copy()
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            def remove_near_duplicates(sec_df_, text_col="content", sim_threshold=FUZZY_SIM_THRESHOLD):
                texts = sec_df_[text_col].tolist()
                vec = TfidfVectorizer(min_df=1, ngram_range=(1,2))
                X = vec.fit_transform(texts)
                keep_idx = []
                for i in range(X.shape[0]):
                    if not keep_idx:
                        keep_idx.append(i); continue
                    sims = cosine_similarity(X[i], X[keep_idx]).ravel()
                    if sims.max() >= sim_threshold:
                        continue
                    keep_idx.append(i)
                return sec_df_.iloc[keep_idx].reset_index(drop=True)
            sec_df = remove_near_duplicates(sec_df)
            keep_fps = set(sec_df["content_fp"].tolist())
            df = df[df["content_fp"].isin(keep_fps)]
        except Exception:
            pass

out_df_sections = df.drop(columns=["content_fp","table_fp"], errors="ignore")
out_df_sections.to_csv(OUT_CSV_SECTIONS, index=False, quoting=csv.QUOTE_MINIMAL)
print(f"Saved sections+tables CSV to: {OUT_CSV_SECTIONS}")

# 5) Merge tables row-wise by headers (per section)
merged_rows = []
for s, tables_info in tables_per_section:
    if not tables_info:
        continue
    merged_tables = merge_tables_by_headers(
        tables_info,
        header_sim_threshold=MERGE_HEADER_SIM_THRESHOLD,
        max_line_gap=MERGE_MAX_LINE_GAP
    )
    for mt in merged_tables:
        merged_rows.append({
            "section_index": s["index"],
            "header_raw": s["header_raw"],
            "header_norm": s["header_norm"],
            "table_caption": mt.get("caption",""),
            "table_markdown": mt.get("table_markdown",""),
            "table_legends": mt.get("legends",""),
            "table_footnotes": mt.get("footnotes",""),
            "table_context_above": mt.get("context_above",""),
            "table_first_header_cells": mt.get("first_header_cells",""),
            "line_start": mt.get("line_start", -1),
            "line_end": mt.get("line_end", -1),
        })

df_merged = pd.DataFrame(merged_rows, columns=[
    "section_index","header_raw","header_norm",
    "table_caption","table_markdown","table_legends","table_footnotes",
    "table_context_above","table_first_header_cells","line_start","line_end"
])
df_merged.to_csv(OUT_CSV_MERGED, index=False, quoting=csv.QUOTE_MINIMAL)
print(f"Saved MERGED tables CSV to: {OUT_CSV_MERGED}")

# 6) Tables-only CSV (raw tables, no section body) — DEDUP BY TABLE MARKDOWN, KEEP LEGENDS/FOOTNOTES
def _normalize_table_md_for_dedupe(md: str) -> str:
    if not isinstance(md, str): return ""
    md = md.replace("\r\n", "\n").strip()
    # collapse runs of spaces/tabs inside lines
    md = "\n".join(re.sub(r"[ \t]+", " ", ln.strip()) for ln in md.split("\n"))
    # collapse multiple blank lines
    md = re.sub(r"\n{3,}", "\n\n", md)
    return md

def _uniq_join(values):
    """Join unique non-empty strings in order, separated by double newlines."""
    seq = [v for v in values if isinstance(v, str) and v.strip()]
    seen, out = set(), []
    for v in seq:
        if v not in seen:
            seen.add(v); out.append(v)
    return "\n\n".join(out)

tables_only_rows = []
for s, tables_info in tables_per_section:
    for ti in (tables_info or []):
        tables_only_rows.append({
            "section_index": s["index"],
            "header_norm": s["header_norm"],
            "table_caption_or_context": ti["caption"] or ti["context_above"],
            "table_markdown": ti["table_markdown"],
            "table_legends": ti["legends"],
            "table_footnotes": ti["footnotes"],
            "first_header_cells": ti["first_header_cells"],
        })

df_tables_only = pd.DataFrame(
    tables_only_rows,
    columns=["section_index","header_norm","table_caption_or_context",
             "table_markdown","table_legends","table_footnotes","first_header_cells"]
)

if not df_tables_only.empty:
    # Normalize markdown for dedupe key
    df_tables_only["__md_norm__"] = df_tables_only["table_markdown"].map(_normalize_table_md_for_dedupe)

    # Group by normalized markdown so duplicates are merged while PRESERVING legends/footnotes
    agg_df = (
        df_tables_only
        .groupby("__md_norm__", as_index=False)
        .agg({
            "section_index": "min",                       # earliest section index
            "header_norm": "first",                       # first header label
            "table_caption_or_context": _uniq_join,       # merge captions/contexts
            "table_markdown": "first",                    # canonical markdown
            "table_legends": _uniq_join,                  # MERGE legends
            "table_footnotes": _uniq_join,                # MERGE footnotes
            "first_header_cells": "first"
        })
        .drop(columns=["__md_norm__"])
    )
    df_tables_only = agg_df

df_tables_only.to_csv(OUT_CSV_TABLES, index=False, quoting=csv.QUOTE_MINIMAL)
print(f"Saved TABLES-ONLY CSV to: {OUT_CSV_TABLES}")